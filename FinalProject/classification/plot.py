from time import time
import matplotlib.pyplot as plt

trainPercetage = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100] # training data percentage

# perceptron for digits
time = [4.196268082, 8.464776993, 12.73568797, 16.91051579, 21.11814904, 25.38081312, 29.57767105, 33.99771786, 38.23259902, 42.25947809]
testAccuracy = [68.76, 74.14, 75.04, 76.1, 78.18, 79.22, 79.06, 80, 79.82, 80.8]
std = [5.585069382, 1.497664849, 1.650151508, 1.349073756, 1.487279395, 1.388164255, 1.252198067, 1.208304597, 0.7049822693, 0.8660254038]


# perceptron for faces
# time = [0.4639101028, 0.9323031902, 1.383772135, 1.823302746, 2.260907888, 2.689082861, 3.152004004, 3.552192688, 3.995384216, 4.417536974]
# testAccuracy = [68.54, 75.62, 82.42, 83.2, 83.6, 85.34, 85.12, 85.98, 86.46, 86.42]
# std = [7.781259538, 2.243211983, 1.202913131, 0.8631338251, 2.165640783, 2.263404515, 2.229798197, 2.076535576, 2.820106381, 1.546609194]

# naive Bayes for digits
# time = [0.6341879368, 0.7790851593, 0.9930491447, 1.089653015, 1.25755024, 1.419877052, 1.591797829, 1.731026173, 1.897032976, 2.093582869]
# testAccuracy = [76.08, 76.62, 76.78, 77.24, 77.26, 77.26, 77.28, 77.24, 77.42, 77.44]
# std = [0.7429670248, 1.213260071, 0.6379655163, 0.7127411872, 0.3646916506, 0.3049590136, 0.228035085, 0.2302172887, 0.1095445115, 0.0894427191]

# naive Bayes for faces
# time = [2.709470987, 2.812730312, 2.863176823, 3.037161827, 3.035547972, 3.065768003, 3.175688028, 3.258857012, 3.296796799, 3.442404032]
# testAccuracy = [70.4, 79.58, 86.14, 86, 86.8, 87.08, 89.18, 88.26, 89.06, 88.14]
# std = [6.81505686, 3.853180504, 2.083986564, 1.3, 1.3, 1.219426095, 1.10770032, 0.7635443668, 0.7503332593, 0.3130495168]

# log regression for digits
# time = [1.763820887, 2.400626898, 3.60422492, 3.837146997, 4.751640797, 5.329993963, 6.875759125, 6.972355843, 7.457113981, 9.137432098]
# testAccuracy = [79.46, 82.28, 82.64, 83.4, 84.48, 84.62, 84.94, 85.7, 85.82, 85.8]
# std = [1.289573573, 0.6534523701, 0.7503332593, 0.8514693183, 0.4868264578, 0.3701351105, 0.4159326869, 0.4415880433, 0.5215361924, 0.4358898944]

# log regression for faces
# time = [0.9111790657, 1.257606268, 1.533348083, 1.682288885, 1.84480381, 1.851242065, 2.054958105, 2.483216047, 2.511002064, 2.666673899]
# testAccuracy = [77.2, 84.26, 87.06, 88.14, 87.6, 88.94, 88.8, 88.26, 88.54, 87.46]
# std = [4.188675208, 1.607171428, 1.778482499, 1.715517415, 2.764959313, 1.778482499, 2.035927307, 1.607171428, 1.266096363, 1.001498877]

# accuracy plot
# plt.title("Perceptron Algorithm for Digits")
# plt.xlabel("Used Training Data (%)")
# plt.ylabel("Average Test Accuracy (%)")
# plt.plot(trainPercetage, testAccuracy)
# plt.xticks(trainPercetage,trainPercetage)
# plt.legend()
# plt.show()

# sd plot
plt.title("Perceptron Algorithm for Digits")
plt.xlabel("Used Training Data (%)")
plt.ylabel("Standard Deviation of Test Accuracy")
plt.plot(trainPercetage, std)
plt.xticks(trainPercetage,trainPercetage)
plt.legend()
plt.show()


# time plot
# plt.title("Log Regression Algorithm for Digits")
# plt.xlabel("Used Training Data (%)")
# plt.ylabel("Training Time (sec)")
# plt.plot(trainPercetage, time)
# plt.xticks(trainPercetage,trainPercetage)
# plt.legend()
# plt.show()